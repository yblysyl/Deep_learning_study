https://www.bilibili.com/video/BV1ho4y1W7qA
神经元--最小的神经网络  
	详见 图1.png
	权重、激活函数、特征、偏置。  特征向量和权重向量相乘(内积)计算后加入偏置值 在用激活函数运算

二分类逻辑斯蒂回归模型 详见 图2.png
神经元>多输出
	W从向量扩展为矩阵
	输出W*x则变成向量
多输出神经元>softmax >多分类逻辑斯蒂回归模型
	详见 图3.png
	其中归一化比如将其x1、x2.....求和（sum）后 将每个值分别x1/sum ......使其总和为一

神经网络训练
	调整参数使模型在训练集上的损失函数最小
		简单的损失函数如 
			将模型值和真实值计算方差  (y-model(x))^2
			交叉熵损失 yIn(model(x))

	梯度下降算法 详见 图4.png 其中等式左边为参数
		学习率 （下降过程中相对与偏导数得步长即▲x）：不应太大也不应太小

Tensorflow
	介绍：google Brain第二代机器学习框架 开源社区活页 可扩展好 API健全
	计算图模型 --声明式编程   先构建计算图，再输入数值进行计算

  神经元Tensorflow搭建
	pip install tensorflow
	https://www.cs.toronto.edu/~kriz/cifar.html --下载数据集
    	----先略过，tensorflow 2.0与1.0不兼容

神经网络
	随机梯度下降
		每次只使用一个样本
	Mini-Batch梯度下降
		每次使用小部分数据进行训练
	问题：梯度下降存在局部极值和saddle point的问题   ---局部极值和局部导数为0
动量梯度下降SGD：
	开始训练时，积累动量，加速训练
	局部极值附近震荡时，梯度为0，由于动量，跳出陷阱
	梯度改变方向的时候，动量缓解动荡
	



神经网络算法：


1、卷积神经网络CNN： （多用于图像）
	常规神经网络问题：参数过多   解决方法：局部连接--图像的局域性 、参数共享--图像特征与位置无关	
	卷积：***
		输入图像和卷积核运算产生输出 （卷积核矩阵和输入图像等大部分做乘法，为输出位置的的值，然后卷积向右或下滑动进行计算） 
		输出size=输入size-卷积核size+1  可以给原图像家padding保持输出等于输入 
		输入5x5  卷积核3x3 结果 3x3 矩阵 
	 卷积多通道：卷积核也增加通道  结果不增加通道为多通道对应位置卷积的和
	 多个卷积核：提取多个特征 输出通道数等于卷积核数目
	激活函数：多种激活函数选取，非线性函数。不同的函数有各自的特点，对结果会产生自身特性相关的影响。
	池化：***
		池化一最大值池化
			默认步长等于卷积核size，卷积核无数据，每次选取卷积核大小区域的最大值为新值
		池化一一平均值池化
			选取区域平均值为新值，其他同上
		特点：
			常使用不重叠、不补零
			没有用于求导的参数
			池化层参数为步长和池化核大小
			用于减少图像尺寸，从而减少计算量
			一定程度解决平移鲁棒
			损失了空间位置精度
	全连接：***
		将上一层输出展开并连接到每一个神经元上
	卷积神经网络=卷积层+池化层+全连接层   卷积层、池化层、全连接层可以有很多层 。但全连接层之后不能再有卷积和池化，因为数据降维了
	全卷积神经网络=卷积层+池化层 --图像到图像，最后可用反卷积（步长小于1）恢复图像大小
	
	模型***：卷积神经网络是一种概念，对于该概念有许多相对应的模型。每个模型都是卷积层，全连接层，池化层的组合，多层嵌套形成一个庞大优质的模型
		模型对于结果的正确率很大，选择或设计好的模型可以提升学习结果和速度。
	


	调参：
		网络初始化：
			全部为0，--单层可以，多层不可以
			其他初始化 --使数据在使用激活函数后分散分布
			保存模型学习结果，载入保存继续学习
	数据增强：
		归一化 ***
		图像变换 
			翻转、拉伸、裁剪、变形
		色彩变换
			对比度、亮度
		多尺度

	更多调参技巧：
		拿到更多的数据
		给神经网络添加层次
		紧跟最新进展，使用新方法
		增大训练的迭代次数
		尝试正则化||w||2
		使用更多的GPU来加速训练
		在标准数据集上训练
		在小数据集上过拟合
		数据集分布平衡
		使用预调整好的稳定模型结构
		Fine-tuning
			预训练好的网络结构上进行微调

	卷积神经网络的常见应用
		图像风格转换 ***   
		图像修复
		换脸
		文本生成图像等

	卷积神经网络的每一层的激活值都可以看做是图像的抽象表示
	卷积神经网络中某层的每个激活值都可以看做是一个分类器，众多的分类结果组成了抽象表示
	使用内容特征生成图像   ---图像风格转换 ***   
		保持卷积神经网络的参数不变
		调整图像x的像素值，使之与y图像在CNN中的内容特征距离变小
	风格特征的计算-Gram矩阵
其中对于很多操作都在tensorflow中封装了元操作(卷积、池化)和各种api(激活函数等)。将元操作组装可得到各种模型和配置，利用api可以加快配置和算法选取。也可以自定义算法
2、循环神经网络RNN：（多用于文本）
	为什么需要循环神经网络一一序列式问题
		多对多，多对一，一对多，实时多对多等卷积序列无法实现
	
	循环神经网络： 详见 循环神经网络1.png
		维护一个状态作为下一步的额外输入
		每一步使用同样的激活函数和参数 St = F(S(t-1),Xt)  ---注小写的为右下角角标
		最简单的循环神经网络 St = Fw(s(t-1),Xt) 。其中展开为 St = tanh(Ws(t-1) + Uxt)   --右边sx为中等角标  tanh为激活函数
	类型
		正向传播
		逆向传播
		双向网络 --可以获得上下文联系
3、长短期记忆网络LSTM：（长久记忆信息）
	参考：https://zhuanlan.zhihu.com/p/123857569
	为什么需要LSTM：
		普通RNN的信息不能长久传播(存在于理论上)
		引入选择性机制
			选择性输出
			选择性输入
			选择性遗忘
		选择性->门
			Sigmoid函数: [0,1]
		结构图： 详见 LSTM神经网络1.png
			状态传递：--图示中最上边的线，依次经过点积、计算和
			遗忘门：新的一句有新的主语，就应该把之前的主语忘掉  --图示左边第一条线
			传入门：是不是要把主语的性别信息添加进来		--图示左边第二条线   ；  tanh --传入的信息 ---图示左边第三条线  
			输出门：动词该用单数形式还是复数形式 --图示左边第四条线  

		LSTM文本分类
			--将文本分词后 转换为向量的格式，在进行计算
			HAN文本分类 --详见 HAN文本分类模型.png


	Embedding压缩
			

文本分类步骤：
	数据预处理：
		分词		 
	 	词表生成与类别表生成

	构建计算图-LSTM模型：
		计算图输入定义
		计算图实现
			embedding  -- 参考 https://zhuanlan.zhihu.com/p/164502624
			LSTM
			fc
			train op
	指标计算与梯度算子实现
	训练流程代码
	数据集封装
	词表封装 --句子转换id类别的封装:
	类别封装


图像生成文本的诸多模型：
	 Multi-Modal RNN 、Show and Tell等等还有很多模型，具体再进行参考
















	